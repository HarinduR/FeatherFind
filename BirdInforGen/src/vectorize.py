import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import json

# ðŸ”¹ File Paths
qa_dataset_file = "../RAG_dataset/qa_dataset.json"
faiss_index_file = "../vector_database/faiss_index.bin"
question_embeddings_file = "../vector_database/question_embeddings.npy"
answer_embeddings_file = "../vector_database/answer_embeddings.npy"
qa_mapping_file = "../vector_database/qa_mapping.npy"

# ðŸ”¹ Load Embedding Model
model = SentenceTransformer("all-MiniLM-L6-v2")

# ðŸ”¹ Load Q&A Dataset
with open(qa_dataset_file, "r", encoding="utf-8") as f:
    dataset = json.load(f)

# ðŸ”¹ Initialize Lists
questions = []
answers = []

# ðŸ”¹ Store Questions and Answers Separately
for entry in dataset:
    questions.append(entry["question"])
    answers.append(entry["retrieved_chunk"])  # Ensures we retrieve specific chunks, NOT full descriptions

# ðŸ”¹ Convert to Embeddings
question_embeddings = model.encode(questions)  # Vectorize questions
answer_embeddings = model.encode(answers)  # Vectorize retrieved chunks

# ðŸ”¹ Create FAISS Index
dimension = question_embeddings.shape[1]
faiss_index = faiss.IndexFlatL2(dimension)

# ðŸ”¹ Add Answer Embeddings (for retrieval)
faiss_index.add(answer_embeddings)

# ðŸ”¹ Save FAISS Index & Embeddings
faiss.write_index(faiss_index, faiss_index_file)
np.save(question_embeddings_file, question_embeddings)
np.save(answer_embeddings_file, answer_embeddings)
np.save(qa_mapping_file, np.array(list(zip(questions, answers)), dtype=object))

print("âœ… FAISS index & embeddings saved successfully!")
