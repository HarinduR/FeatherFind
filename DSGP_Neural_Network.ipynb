{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPV/ZHkrRSEsHofHJyh+cMR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarinduR/FeatherFind/blob/Keyword-Bird-Finder/DSGP_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn\n",
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylWo8LbJTaZb",
        "outputId": "dfcd672f-0cf0-4339-9a54-92e23b9d16c4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKp1hd2I2E9f",
        "outputId": "a3088af7-6aa5-495c-dd9a-ac87596ed07d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.11/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (24.2)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons) (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from sklearn.base import clone\n",
        "\n",
        "class MultiLabelSMOTE:\n",
        "    def __init__(self, sampler=SMOTE(), classifier=LogisticRegression()):\n",
        "        self.sampler = sampler\n",
        "        self.classifier = classifier\n",
        "        self.models = {}\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        # Apply SMOTE to each label independently\n",
        "        self.models = {}\n",
        "        for label_idx in range(y.shape[1]):\n",
        "            sampler = clone(self.sampler)\n",
        "            X_res, y_res = sampler.fit_resample(X, y[:, label_idx])\n",
        "            model = clone(self.classifier).fit(X_res, y_res)\n",
        "            self.models[label_idx] = model\n",
        "        return X, y  # Returns original data but stores per-label models\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.zeros((X.shape[0], len(self.models)))\n",
        "        for idx, model in self.models.items():\n",
        "            preds[:, idx] = model.predict_proba(X)[:, 1]\n",
        "        return preds"
      ],
      "metadata": {
        "id": "UJhjfI0uyAii"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First install required package\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Load and prepare data\n",
        "df = pd.read_csv(\"/content/bird_features_full_dataset.csv\")\n",
        "label_columns = [\"Size\", \"Primary Color\", \"Secondary Color\", \"Habitat\", \"Region\",\n",
        "                \"Diet\", \"Beak Size\", \"Beak Color\", \"Legs Size\", \"Legs Color\",\n",
        "                \"Eyes Size\", \"Eyes Color\"]\n",
        "\n",
        "# Convert labels to binary matrix\n",
        "mlb = MultiLabelBinarizer()\n",
        "y = mlb.fit_transform(df[label_columns].apply(lambda x: [f\"{k}={v}\" for k,v in x.items() if v != \"none\"], axis=1))\n",
        "X = df[\"Description\"]\n",
        "\n",
        "# Split data (preserve original splits)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "7YXPpkKvyP56"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "class SMOTEWrapper(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, sampling_strategy='minority'):\n",
        "        self.sampler = SMOTE(sampling_strategy=sampling_strategy)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # SMOTE requires y to be 1D, so we handle each label separately\n",
        "        self.samplers_ = []\n",
        "        for label_idx in range(y.shape[1]):\n",
        "            sampler = clone(self.sampler)\n",
        "            sampler.fit_resample(X, y[:, label_idx])\n",
        "            self.samplers_.append(sampler)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Return original X since SMOTE is only applied during resampling\n",
        "        return X\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        # Apply SMOTE to each label independently\n",
        "        X_resampled, y_resampled = [], []\n",
        "        for label_idx, sampler in enumerate(self.samplers_):\n",
        "            X_res, y_res = sampler.fit_resample(X, y[:, label_idx])\n",
        "            X_resampled.append(X_res)\n",
        "            y_resampled.append(y_res)\n",
        "\n",
        "        # Combine results\n",
        "        X_combined = np.vstack(X_resampled)\n",
        "        y_combined = np.vstack(y_resampled)\n",
        "        return X_combined, y_combined"
      ],
      "metadata": {
        "id": "gbeMZl5WzmHp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=500)),\n",
        "    ('smote', SMOTEWrapper(sampling_strategy='minority')),\n",
        "    ('clf', MultiOutputClassifier(LogisticRegression(class_weight='balanced')))\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9h-m2mSznbB",
        "outputId": "53efd544-3186-4534-d77b-7c2e4631b6b8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.50      0.20        16\n",
            "           1       0.12      0.43      0.19        14\n",
            "           2       0.09      0.50      0.15        12\n",
            "           3       0.11      0.47      0.17        15\n",
            "           4       0.11      0.56      0.18        16\n",
            "           5       0.48      0.51      0.49        91\n",
            "           6       0.05      0.44      0.10         9\n",
            "           7       0.05      0.27      0.08        11\n",
            "           8       0.17      0.39      0.23        18\n",
            "           9       0.02      0.12      0.03         8\n",
            "          10       0.31      0.36      0.33        25\n",
            "          11       0.30      0.88      0.45        24\n",
            "          12       0.56      0.85      0.68        78\n",
            "          13       0.40      0.73      0.51        26\n",
            "          14       0.42      0.76      0.54        29\n",
            "          15       0.38      0.71      0.49        28\n",
            "          16       0.33      0.74      0.46        19\n",
            "          17       0.47      0.78      0.58        18\n",
            "          18       0.59      0.76      0.67        29\n",
            "          19       0.62      0.73      0.67        55\n",
            "          20       0.50      0.79      0.61        29\n",
            "          21       0.45      0.72      0.55        25\n",
            "          22       0.26      0.56      0.36        18\n",
            "          23       0.45      0.59      0.51        17\n",
            "          24       0.16      0.48      0.24        21\n",
            "          25       0.14      0.47      0.21        17\n",
            "          26       0.08      0.36      0.12        14\n",
            "          27       0.10      0.50      0.17        10\n",
            "          28       0.08      0.45      0.14        11\n",
            "          29       0.42      0.58      0.49        79\n",
            "          30       0.13      0.53      0.21        15\n",
            "          31       0.05      0.38      0.08         8\n",
            "          32       0.11      0.38      0.17        16\n",
            "          33       0.19      0.68      0.30        19\n",
            "          34       0.26      0.44      0.32        39\n",
            "          35       0.27      0.47      0.34        43\n",
            "          36       0.38      0.52      0.44        83\n",
            "          37       0.26      0.47      0.33        45\n",
            "          38       0.55      0.80      0.65        20\n",
            "          39       0.52      0.80      0.63        15\n",
            "          40       0.52      0.86      0.65        14\n",
            "          41       0.39      0.89      0.54        19\n",
            "          42       0.56      0.69      0.62        13\n",
            "          43       0.37      0.80      0.51        20\n",
            "          44       0.67      0.83      0.74        65\n",
            "          45       0.42      0.88      0.57        17\n",
            "          46       0.79      0.70      0.75        27\n",
            "          47       0.11      0.56      0.19        16\n",
            "          48       0.10      0.43      0.16        14\n",
            "          49       0.17      0.62      0.27        16\n",
            "          50       0.12      0.53      0.20        15\n",
            "          51       0.12      0.53      0.20        15\n",
            "          52       0.34      0.48      0.40        62\n",
            "          53       0.12      0.32      0.18        22\n",
            "          54       0.26      0.62      0.37        24\n",
            "          55       0.14      0.50      0.22        14\n",
            "          56       0.17      0.83      0.28        12\n",
            "          57       0.41      0.80      0.54        46\n",
            "          58       0.38      0.61      0.47        44\n",
            "          59       0.49      0.55      0.52        78\n",
            "          60       0.36      0.64      0.46        42\n",
            "          61       0.31      0.72      0.43        18\n",
            "          62       0.35      0.93      0.51        14\n",
            "          63       0.39      0.76      0.52        17\n",
            "          64       0.28      0.85      0.42        13\n",
            "          65       0.31      0.69      0.42        16\n",
            "          66       0.33      0.60      0.42        40\n",
            "          67       0.55      0.88      0.68        24\n",
            "          68       0.42      0.87      0.57        15\n",
            "          69       0.43      0.76      0.55        17\n",
            "          70       0.46      0.81      0.59        16\n",
            "          71       0.38      0.75      0.51        20\n",
            "          72       0.30      0.78      0.44        18\n",
            "          73       0.28      0.76      0.41        17\n",
            "          74       0.28      0.53      0.36        19\n",
            "          75       0.31      0.50      0.38        26\n",
            "          76       0.19      0.75      0.31        16\n",
            "          77       0.24      0.57      0.33        21\n",
            "          78       0.30      0.72      0.42        18\n",
            "          79       0.60      0.65      0.63        75\n",
            "          80       0.06      0.50      0.11         6\n",
            "          81       0.06      0.75      0.11         4\n",
            "          82       0.14      1.00      0.25         7\n",
            "          83       0.10      0.42      0.17        12\n",
            "          84       0.08      0.36      0.13        11\n",
            "          85       0.64      0.55      0.59       123\n",
            "          86       0.08      0.40      0.13        10\n",
            "          87       0.10      0.45      0.17        11\n",
            "          88       0.07      0.44      0.13         9\n",
            "          89       0.14      0.56      0.22         9\n",
            "          90       0.10      0.62      0.17         8\n",
            "          91       0.75      0.84      0.79        32\n",
            "          92       0.67      0.80      0.73        35\n",
            "          93       0.51      0.83      0.63        29\n",
            "          94       0.45      0.73      0.56        48\n",
            "          95       0.37      0.96      0.53        25\n",
            "          96       0.69      0.83      0.76        41\n",
            "\n",
            "   micro avg       0.30      0.64      0.41      2520\n",
            "   macro avg       0.31      0.63      0.39      2520\n",
            "weighted avg       0.39      0.64      0.47      2520\n",
            " samples avg       0.30      0.64      0.41      2520\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=500)\n",
        "X_train_vec = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_vec = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Verify shapes\n",
        "print(f\"Training data shape: {X_train_vec.shape}\")  # Should be (num_samples, 500)\n",
        "print(f\"Test data shape: {X_test_vec.shape}\")        # Should be (num_samples, 500)\n",
        "print(f\"y_train shape: {y_train.shape}\")             # Should be (num_samples, num_labels)\n",
        "\n",
        "# Calculate class weights\n",
        "pos_weights = (len(y_train) - y_train.sum(axis=0)) / y_train.sum(axis=0)\n",
        "class_weights = {i: weight for i, weight in enumerate(pos_weights)}\n",
        "\n",
        "# Build a simple neural network\n",
        "input_shape = X_train_vec.shape[1]\n",
        "num_labels = y_train.shape[1]\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "    Dropout(0.5),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_labels, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_vec, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test_vec, y_test),\n",
        "    class_weight=class_weights\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred = (model.predict(X_test_vec) > 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9evdBkT3ISG",
        "outputId": "dda29612-4e3c-470e-d9e5-e2c8a31dd199"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (840, 129)\n",
            "Test data shape: (210, 129)\n",
            "y_train shape: (840, 97)\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.0095 - loss: 6.0660 - val_accuracy: 0.0048 - val_loss: 0.5900\n",
            "Epoch 2/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0017 - loss: 4.6940 - val_accuracy: 0.0000e+00 - val_loss: 0.3622\n",
            "Epoch 3/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 3.3886 - val_accuracy: 0.0000e+00 - val_loss: 0.3481\n",
            "Epoch 4/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 3.4134 - val_accuracy: 0.0000e+00 - val_loss: 0.3458\n",
            "Epoch 5/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 3.2701 - val_accuracy: 0.0000e+00 - val_loss: 0.3458\n",
            "Epoch 6/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 3.3575 - val_accuracy: 0.0000e+00 - val_loss: 0.3458\n",
            "Epoch 7/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.1874 - val_accuracy: 0.0000e+00 - val_loss: 0.3454\n",
            "Epoch 8/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.2068 - val_accuracy: 0.0000e+00 - val_loss: 0.3453\n",
            "Epoch 9/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.2521 - val_accuracy: 0.0000e+00 - val_loss: 0.3452\n",
            "Epoch 10/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.2719 - val_accuracy: 0.0000e+00 - val_loss: 0.3447\n",
            "Epoch 11/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.1771 - val_accuracy: 0.0000e+00 - val_loss: 0.3444\n",
            "Epoch 12/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.2327 - val_accuracy: 0.0000e+00 - val_loss: 0.3441\n",
            "Epoch 13/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.1793 - val_accuracy: 0.0000e+00 - val_loss: 0.3437\n",
            "Epoch 14/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.2516 - val_accuracy: 0.0000e+00 - val_loss: 0.3434\n",
            "Epoch 15/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.1821 - val_accuracy: 0.0000e+00 - val_loss: 0.3434\n",
            "Epoch 16/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.2079 - val_accuracy: 0.0000e+00 - val_loss: 0.3433\n",
            "Epoch 17/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.0847 - val_accuracy: 0.0000e+00 - val_loss: 0.3430\n",
            "Epoch 18/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.0020 - val_accuracy: 0.0000e+00 - val_loss: 0.3424\n",
            "Epoch 19/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0000e+00 - loss: 3.1063 - val_accuracy: 0.0000e+00 - val_loss: 0.3421\n",
            "Epoch 20/20\n",
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.0000e+00 - loss: 3.1425 - val_accuracy: 0.0000e+00 - val_loss: 0.3417\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        16\n",
            "           1       0.00      0.00      0.00        14\n",
            "           2       0.00      0.00      0.00        12\n",
            "           3       0.00      0.00      0.00        15\n",
            "           4       0.00      0.00      0.00        16\n",
            "           5       0.00      0.00      0.00        91\n",
            "           6       0.00      0.00      0.00         9\n",
            "           7       0.00      0.00      0.00        11\n",
            "           8       0.00      0.00      0.00        18\n",
            "           9       0.00      0.00      0.00         8\n",
            "          10       0.00      0.00      0.00        25\n",
            "          11       0.00      0.00      0.00        24\n",
            "          12       0.00      0.00      0.00        78\n",
            "          13       0.00      0.00      0.00        26\n",
            "          14       0.00      0.00      0.00        29\n",
            "          15       0.00      0.00      0.00        28\n",
            "          16       0.00      0.00      0.00        19\n",
            "          17       0.00      0.00      0.00        18\n",
            "          18       0.00      0.00      0.00        29\n",
            "          19       0.00      0.00      0.00        55\n",
            "          20       0.00      0.00      0.00        29\n",
            "          21       0.00      0.00      0.00        25\n",
            "          22       0.00      0.00      0.00        18\n",
            "          23       0.00      0.00      0.00        17\n",
            "          24       0.00      0.00      0.00        21\n",
            "          25       0.00      0.00      0.00        17\n",
            "          26       0.00      0.00      0.00        14\n",
            "          27       0.00      0.00      0.00        10\n",
            "          28       0.00      0.00      0.00        11\n",
            "          29       0.00      0.00      0.00        79\n",
            "          30       0.00      0.00      0.00        15\n",
            "          31       0.00      0.00      0.00         8\n",
            "          32       0.00      0.00      0.00        16\n",
            "          33       0.00      0.00      0.00        19\n",
            "          34       0.00      0.00      0.00        39\n",
            "          35       0.00      0.00      0.00        43\n",
            "          36       0.00      0.00      0.00        83\n",
            "          37       0.00      0.00      0.00        45\n",
            "          38       0.00      0.00      0.00        20\n",
            "          39       0.00      0.00      0.00        15\n",
            "          40       0.00      0.00      0.00        14\n",
            "          41       0.00      0.00      0.00        19\n",
            "          42       0.00      0.00      0.00        13\n",
            "          43       0.00      0.00      0.00        20\n",
            "          44       0.00      0.00      0.00        65\n",
            "          45       0.00      0.00      0.00        17\n",
            "          46       0.00      0.00      0.00        27\n",
            "          47       0.00      0.00      0.00        16\n",
            "          48       0.00      0.00      0.00        14\n",
            "          49       0.00      0.00      0.00        16\n",
            "          50       0.00      0.00      0.00        15\n",
            "          51       0.00      0.00      0.00        15\n",
            "          52       0.00      0.00      0.00        62\n",
            "          53       0.00      0.00      0.00        22\n",
            "          54       0.00      0.00      0.00        24\n",
            "          55       0.00      0.00      0.00        14\n",
            "          56       0.00      0.00      0.00        12\n",
            "          57       0.00      0.00      0.00        46\n",
            "          58       0.00      0.00      0.00        44\n",
            "          59       0.00      0.00      0.00        78\n",
            "          60       0.00      0.00      0.00        42\n",
            "          61       0.00      0.00      0.00        18\n",
            "          62       0.00      0.00      0.00        14\n",
            "          63       0.00      0.00      0.00        17\n",
            "          64       0.00      0.00      0.00        13\n",
            "          65       0.00      0.00      0.00        16\n",
            "          66       0.00      0.00      0.00        40\n",
            "          67       0.00      0.00      0.00        24\n",
            "          68       0.00      0.00      0.00        15\n",
            "          69       0.00      0.00      0.00        17\n",
            "          70       0.00      0.00      0.00        16\n",
            "          71       0.00      0.00      0.00        20\n",
            "          72       0.00      0.00      0.00        18\n",
            "          73       0.00      0.00      0.00        17\n",
            "          74       0.00      0.00      0.00        19\n",
            "          75       0.00      0.00      0.00        26\n",
            "          76       0.00      0.00      0.00        16\n",
            "          77       0.00      0.00      0.00        21\n",
            "          78       0.00      0.00      0.00        18\n",
            "          79       0.00      0.00      0.00        75\n",
            "          80       0.00      0.00      0.00         6\n",
            "          81       0.00      0.00      0.00         4\n",
            "          82       0.00      0.00      0.00         7\n",
            "          83       0.00      0.00      0.00        12\n",
            "          84       0.00      0.00      0.00        11\n",
            "          85       0.59      0.94      0.72       123\n",
            "          86       0.00      0.00      0.00        10\n",
            "          87       0.00      0.00      0.00        11\n",
            "          88       0.00      0.00      0.00         9\n",
            "          89       0.00      0.00      0.00         9\n",
            "          90       0.00      0.00      0.00         8\n",
            "          91       0.00      0.00      0.00        32\n",
            "          92       0.00      0.00      0.00        35\n",
            "          93       0.00      0.00      0.00        29\n",
            "          94       0.00      0.00      0.00        48\n",
            "          95       0.00      0.00      0.00        25\n",
            "          96       0.00      0.00      0.00        41\n",
            "\n",
            "   micro avg       0.59      0.05      0.09      2520\n",
            "   macro avg       0.01      0.01      0.01      2520\n",
            "weighted avg       0.03      0.05      0.04      2520\n",
            " samples avg       0.55      0.05      0.08      2520\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "8hIzEVGe11II",
        "outputId": "f636f4e6-8878-4285-f596-e8f734a03ae5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}