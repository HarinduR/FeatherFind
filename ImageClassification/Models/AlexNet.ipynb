{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDySIRkVkF-o",
        "outputId": "9ca2e739-b736-4f0b-b8b0-d65827246290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzipping /content/augmented_images.zip to /content/extracted_images/augmented_images...\n",
            "Finished unzipping /content/augmented_images.zip to /content/extracted_images/augmented_images\n",
            "Unzipping /content/augmented_images(white throated kingfisher).zip to /content/extracted_images/augmented_images(white throated kingfisher)...\n",
            "Finished unzipping /content/augmented_images(white throated kingfisher).zip to /content/extracted_images/augmented_images(white throated kingfisher)\n",
            "Unzipping /content/augmented_images(red vented bulbul).zip to /content/extracted_images/augmented_images(red vented bulbul)...\n",
            "Finished unzipping /content/augmented_images(red vented bulbul).zip to /content/extracted_images/augmented_images(red vented bulbul)\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Paths to your zip files\n",
        "zip_paths = [\n",
        "    \"/content/augmented_images.zip\",\n",
        "    \"/content/augmented_images(white throated kingfisher).zip\",\n",
        "    \"/content/augmented_images(red vented bulbul).zip\"\n",
        "]\n",
        "\n",
        "# Unzipping function\n",
        "def unzip_files(zip_paths, extract_to_base):\n",
        "    for zip_path in zip_paths:\n",
        "        # Get the folder name based on the zip file name\n",
        "        folder_name = os.path.splitext(os.path.basename(zip_path))[0]\n",
        "        extract_to = os.path.join(extract_to_base, folder_name)\n",
        "\n",
        "        os.makedirs(extract_to, exist_ok=True)\n",
        "        print(f\"Unzipping {zip_path} to {extract_to}...\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Finished unzipping {zip_path} to {extract_to}\")\n",
        "\n",
        "# Unzip all files to separate directories for each bird\n",
        "extracted_base_dir = \"/content/extracted_images\"\n",
        "unzip_files(zip_paths, extracted_base_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Function to split and organize datasets\n",
        "def split_and_organize_datasets(base_extracted_dir, split_base_dir, test_size=0.2, val_size=0.2):\n",
        "    # Clear or create split directories\n",
        "    if os.path.exists(split_base_dir):\n",
        "        shutil.rmtree(split_base_dir)\n",
        "    os.makedirs(split_base_dir, exist_ok=True)\n",
        "\n",
        "    # Create subdirectories for train, validation, and test\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(split_base_dir, split), exist_ok=True)\n",
        "\n",
        "    # Loop through each bird folder and split its images\n",
        "    for bird_folder in os.listdir(base_extracted_dir):\n",
        "        bird_dir = os.path.join(base_extracted_dir, bird_folder)\n",
        "        if not os.path.isdir(bird_dir):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {bird_folder}...\")\n",
        "\n",
        "        # Get all image files in the current bird folder\n",
        "        images = os.listdir(bird_dir)\n",
        "        images = [img for img in images if img.endswith(('.jpg', '.jpeg', '.png'))]  # Filter valid image files\n",
        "\n",
        "        if len(images) == 0:\n",
        "            print(f\"No valid image files found in {bird_folder}\")\n",
        "            continue\n",
        "\n",
        "        # Split the images into train, test, and validation sets\n",
        "        train_imgs, test_imgs = train_test_split(images, test_size=test_size, random_state=42)\n",
        "        train_imgs, val_imgs = train_test_split(train_imgs, test_size=val_size, random_state=42)\n",
        "\n",
        "        # Copy images to the corresponding directories\n",
        "        for split, imgs in zip(['train', 'val', 'test'], [train_imgs, val_imgs, test_imgs]):\n",
        "            split_dir = os.path.join(split_base_dir, split, bird_folder)\n",
        "            os.makedirs(split_dir, exist_ok=True)\n",
        "            for img in imgs:\n",
        "                shutil.copy(os.path.join(bird_dir, img), os.path.join(split_dir, img))\n",
        "\n",
        "# Path to your extracted dataset directory\n",
        "base_extracted_dir = \"/content/extracted_images\"\n",
        "split_base_dir = \"/content/split_datasets\"\n",
        "split_and_organize_datasets(base_extracted_dir, split_base_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cIFyxSVkd2Z",
        "outputId": "3f7b1677-5fb6-457b-8d6b-01f6024c6705"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing augmented_images...\n",
            "Processing augmented_images(red vented bulbul)...\n",
            "Processing augmented_images(white throated kingfisher)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Image size and batch size settings\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "\n",
        "# Create the ImageDataGenerator objects for train, validation, and test sets\n",
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
        "\n",
        "# Define the directories for train, validation, and test\n",
        "train_dir = os.path.join(\"/content/split_datasets\", 'train')\n",
        "val_dir = os.path.join(\"/content/split_datasets\", 'val')\n",
        "test_dir = os.path.join(\"/content/split_datasets\", 'test')\n",
        "\n",
        "# Create data generators for train, validation, and test\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgjZc5m3kf7D",
        "outputId": "90481dba-0ef4-4fe1-a33c-7ec019693719"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1920 images belonging to 3 classes.\n",
            "Found 480 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define AlexNet Architecture\n",
        "def create_alexnet_model(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Layer 1: Conv + ReLU + MaxPooling\n",
        "    model.add(Conv2D(96, kernel_size=(11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "    # Layer 2: Conv + ReLU + MaxPooling\n",
        "    model.add(Conv2D(256, kernel_size=(5, 5), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "    # Layer 3: Conv + ReLU\n",
        "    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # Layer 4: Conv + ReLU\n",
        "    model.add(Conv2D(384, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # Layer 5: Conv + ReLU + MaxPooling\n",
        "    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
        "\n",
        "    # Flattening\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layers\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create the AlexNet model\n",
        "input_shape = (224, 224, 3)  # Image shape\n",
        "num_classes = train_gen.num_classes  # Number of bird categories\n",
        "model = create_alexnet_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgbQwT92lNwL",
        "outputId": "522dd8ce-afb2-41e4-b9ed-7015d0c18d1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1920 images belonging to 3 classes.\n",
            "Found 480 images belonging to 3 classes.\n",
            "Found 600 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFKzLBp_lR3u",
        "outputId": "93751aad-7a54-4d33-afb8-8930be3da023"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 6s/step - accuracy: 0.3429 - loss: 1.1009 - val_accuracy: 0.4750 - val_loss: 1.0765\n",
            "Epoch 2/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m367s\u001b[0m 6s/step - accuracy: 0.4858 - loss: 1.0549 - val_accuracy: 0.5458 - val_loss: 0.9339\n",
            "Epoch 3/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 6s/step - accuracy: 0.6137 - loss: 0.8390 - val_accuracy: 0.6979 - val_loss: 0.6987\n",
            "Epoch 4/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m391s\u001b[0m 6s/step - accuracy: 0.8057 - loss: 0.5196 - val_accuracy: 0.8146 - val_loss: 0.4368\n",
            "Epoch 5/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 6s/step - accuracy: 0.8809 - loss: 0.3155 - val_accuracy: 0.8708 - val_loss: 0.3562\n",
            "Epoch 6/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 6s/step - accuracy: 0.9194 - loss: 0.2158 - val_accuracy: 0.8792 - val_loss: 0.3590\n",
            "Epoch 7/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 6s/step - accuracy: 0.9345 - loss: 0.1873 - val_accuracy: 0.8896 - val_loss: 0.3211\n",
            "Epoch 8/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m342s\u001b[0m 6s/step - accuracy: 0.9520 - loss: 0.1172 - val_accuracy: 0.9083 - val_loss: 0.2565\n",
            "Epoch 9/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 6s/step - accuracy: 0.9711 - loss: 0.0793 - val_accuracy: 0.9000 - val_loss: 0.3230\n",
            "Epoch 10/10\n",
            "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 6s/step - accuracy: 0.9728 - loss: 0.0738 - val_accuracy: 0.9167 - val_loss: 0.2580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average training accuracy across all epochs\n",
        "average_training_accuracy = sum(history.history['accuracy']) / len(history.history['accuracy'])\n",
        "\n",
        "# Print the result\n",
        "print(f\"Average Training Accuracy: {average_training_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v13Ijq8K2Z1s",
        "outputId": "c9fca70c-870c-47f9-e4be-9efc2de131c2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Accuracy: 0.7931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Evaluate on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_gen, verbose=1)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate classification report\n",
        "y_true = test_gen.classes  # Ground truth labels\n",
        "y_pred = np.argmax(model.predict(test_gen), axis=-1)  # Predicted labels\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=test_gen.class_indices.keys()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Vp5d1blYnV",
        "outputId": "84351495-517b-4e6e-d985-6ba6cc91d670"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step - accuracy: 0.9268 - loss: 0.2650\n",
            "Test Accuracy: 91.67%\n",
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step\n",
            "Classification Report:\n",
            "                                             precision    recall  f1-score   support\n",
            "\n",
            "                           augmented_images       0.92      0.95      0.93       200\n",
            "        augmented_images(red vented bulbul)       0.90      0.89      0.90       200\n",
            "augmented_images(white throated kingfisher)       0.93      0.91      0.92       200\n",
            "\n",
            "                                   accuracy                           0.92       600\n",
            "                                  macro avg       0.92      0.92      0.92       600\n",
            "                               weighted avg       0.92      0.92      0.92       600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}